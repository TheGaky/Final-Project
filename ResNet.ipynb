{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8531dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "# Device configuration\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Hyper-parameters \n",
    "epochs = 2000\n",
    "image_size = (360, 640)\n",
    "batch_size = 16\n",
    "learning_rate = 0.0001\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d2155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, labels_arr, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.DataFrame(labels_arr)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = torch.tensor(int(self.img_labels.iloc[idx, 1]))\n",
    "        if self.transform:\n",
    "            image = self.transform(image.float())\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "    \n",
    "def create_dataset(folder, label):\n",
    "    files = os.listdir(folder)\n",
    "    for i in files:\n",
    "        if i =='.DS_Store':\n",
    "            files.remove(i)\n",
    "    files = np.array(files)\n",
    "    labels_arr = np.zeros(len(files), dtype=np.uint8)\n",
    "    labels_arr.fill(label)\n",
    "    labels_arr = np.column_stack((files, labels_arr))\n",
    "\n",
    "    transform = transforms.Compose([transforms.Resize(image_size)])\n",
    "    dataset = CustomImageDataset(labels_arr, folder, transform=transform)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e21feee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n"
     ]
    }
   ],
   "source": [
    "#pure train datasets\n",
    "dataset_low = create_dataset(\"/Users/gaky/Desktop/efir/on for use/true low\", int(0))\n",
    "dataset_off = create_dataset(\"/Users/gaky/Desktop/efir/off for use/off mix\", int(1))\n",
    "\n",
    "\n",
    "#validation datasets\n",
    "dataset_off_val = create_dataset(\"/Users/gaky/Desktop/efir/off for use/validation off mix\", int(1))\n",
    "dataset_low_val = create_dataset(\"/Users/gaky/Desktop/efir/on for use/low valid\", int(0))\n",
    "\n",
    "train_dataset = ConcatDataset([dataset_low, dataset_off])\n",
    "print(len(train_dataset))\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloader_val_off = DataLoader(dataset_off_val, batch_size=len(dataset_off_val), shuffle=False)\n",
    "dataloader_low_val = DataLoader(dataset_low_val, batch_size=len(dataset_low_val), shuffle=False)\n",
    "\n",
    "dataloader_off = DataLoader(dataset_off, batch_size=len(dataset_off), shuffle=False)\n",
    "dataloader_low = DataLoader(dataset_low, batch_size=len(dataset_low), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b6f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels))\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b719029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU())\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512*6*14, 3)\n",
    "        \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            \n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 512*6*14)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed09e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.001, momentum = 0.9)\n",
    "#optim = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233cff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch):\n",
    "    model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "    model_scripted.save(\"/Users/gaky/Desktop/efir/Models/strange model/\"+ str(epoch) + \".pth\") # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea978be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model2(epoch):\n",
    "    torch.save(model.state_dict(), \"/Users/gaky/Desktop/efir/Models/binary resnet\"+ str(epoch) + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1d713f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, name):\n",
    "    with torch.no_grad():\n",
    "        dataset = iter(dataloader)\n",
    "        x, y = next(dataset)\n",
    "        \n",
    "        x = x/255.\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        model.eval()\n",
    "        out = model(x.float())\n",
    "        cat = torch.argmax(out, dim=1)\n",
    "        \n",
    "        if name == \"off_val\":\n",
    "            for i in range(len(cat)):\n",
    "                if cat[i] == 2:\n",
    "                    cat[i] = 1\n",
    "\n",
    "        accuracy = (cat == y).float().mean()\n",
    "        print(accuracy, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10354c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate2(dataloader, name):\n",
    "    with torch.no_grad():\n",
    "        dick = []\n",
    "        for i, (x, y) in enumerate(dataloader):\n",
    "        \n",
    "            x = x/255.\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            model.eval()\n",
    "            out = model(x.float())\n",
    "            cat = torch.argmax(out, dim=1)\n",
    "\n",
    "\n",
    "            if name == \"off\":\n",
    "                for i in range(len(cat)):\n",
    "                    if cat[i] == 2:\n",
    "                        cat[i] = 1\n",
    "                        \n",
    "            accuracy = (cat == y).float()\n",
    "            dick.append(accuracy.mean())\n",
    "        print(np.mean(dick), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "759aabac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 tensor(1.3890, grad_fn=<MeanBackward0>)\n",
      "0 1 tensor(0.7621, grad_fn=<MeanBackward0>)\n",
      "0 2 tensor(0.6846, grad_fn=<MeanBackward0>)\n",
      "0 3 tensor(1.0051, grad_fn=<MeanBackward0>)\n",
      "0 4 tensor(1.5483, grad_fn=<MeanBackward0>)\n",
      "0 5 tensor(0.7831, grad_fn=<MeanBackward0>)\n",
      "0 6 tensor(1.1529, grad_fn=<MeanBackward0>)\n",
      "0 7 tensor(0.8137, grad_fn=<MeanBackward0>)\n",
      "0 8 tensor(0.8617, grad_fn=<MeanBackward0>)\n",
      "0 9 tensor(0.9407, grad_fn=<MeanBackward0>)\n",
      "0 10 tensor(0.8307, grad_fn=<MeanBackward0>)\n",
      "0 11 tensor(0.7232, grad_fn=<MeanBackward0>)\n",
      "0 12 tensor(1.1706, grad_fn=<MeanBackward0>)\n",
      "0 13 tensor(1.0144, grad_fn=<MeanBackward0>)\n",
      "0 14 tensor(1.4361, grad_fn=<MeanBackward0>)\n",
      "0 15 tensor(1.1346, grad_fn=<MeanBackward0>)\n",
      "0 16 tensor(0.5602, grad_fn=<MeanBackward0>)\n",
      "0 17 tensor(0.5712, grad_fn=<MeanBackward0>)\n",
      "0 18 tensor(1.1212, grad_fn=<MeanBackward0>)\n",
      "5.9977030754089355\n",
      "0.9738652110099792\n",
      "1 0 tensor(0.8365, grad_fn=<MeanBackward0>)\n",
      "tensor(0.) off_val\n",
      "tensor(1.) low_val\n",
      "0.0 off\n",
      "1.0 low\n",
      "1 1 tensor(0.8206, grad_fn=<MeanBackward0>)\n",
      "1 2 tensor(1.0966, grad_fn=<MeanBackward0>)\n",
      "1 3 tensor(0.5102, grad_fn=<MeanBackward0>)\n",
      "1 4 tensor(1.2560, grad_fn=<MeanBackward0>)\n",
      "1 5 tensor(0.7303, grad_fn=<MeanBackward0>)\n",
      "1 6 tensor(0.4919, grad_fn=<MeanBackward0>)\n",
      "1 7 tensor(0.9276, grad_fn=<MeanBackward0>)\n",
      "1 8 tensor(0.5210, grad_fn=<MeanBackward0>)\n",
      "1 9 tensor(1.2903, grad_fn=<MeanBackward0>)\n",
      "1 10 tensor(0.5730, grad_fn=<MeanBackward0>)\n",
      "1 11 tensor(1.1332, grad_fn=<MeanBackward0>)\n",
      "1 12 tensor(1.0045, grad_fn=<MeanBackward0>)\n",
      "1 13 tensor(2.0591, grad_fn=<MeanBackward0>)\n",
      "1 14 tensor(0.8148, grad_fn=<MeanBackward0>)\n",
      "1 15 tensor(0.4617, grad_fn=<MeanBackward0>)\n",
      "1 16 tensor(0.7806, grad_fn=<MeanBackward0>)\n",
      "1 17 tensor(0.4669, grad_fn=<MeanBackward0>)\n",
      "1 18 tensor(0.5410, grad_fn=<MeanBackward0>)\n",
      "5.88481593132019\n",
      "0.9162960428940622\n",
      "2 0 tensor(0.3672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.) off_val\n",
      "tensor(1.) low_val\n",
      "0.0 off\n",
      "1.0 low\n",
      "2 1 tensor(0.6751, grad_fn=<MeanBackward0>)\n",
      "2 2 tensor(0.3811, grad_fn=<MeanBackward0>)\n",
      "2 3 tensor(0.7329, grad_fn=<MeanBackward0>)\n",
      "2 4 tensor(0.5642, grad_fn=<MeanBackward0>)\n",
      "2 5 tensor(0.4690, grad_fn=<MeanBackward0>)\n",
      "2 6 tensor(0.4121, grad_fn=<MeanBackward0>)\n",
      "2 7 tensor(0.5716, grad_fn=<MeanBackward0>)\n",
      "2 8 tensor(0.2746, grad_fn=<MeanBackward0>)\n",
      "2 9 tensor(0.6783, grad_fn=<MeanBackward0>)\n",
      "2 10 tensor(0.3643, grad_fn=<MeanBackward0>)\n",
      "2 11 tensor(0.2702, grad_fn=<MeanBackward0>)\n",
      "2 12 tensor(0.4278, grad_fn=<MeanBackward0>)\n",
      "2 13 tensor(0.5190, grad_fn=<MeanBackward0>)\n",
      "2 14 tensor(0.4024, grad_fn=<MeanBackward0>)\n",
      "2 15 tensor(0.5452, grad_fn=<MeanBackward0>)\n",
      "2 16 tensor(0.4001, grad_fn=<MeanBackward0>)\n",
      "2 17 tensor(0.3960, grad_fn=<MeanBackward0>)\n",
      "2 18 tensor(0.6977, grad_fn=<MeanBackward0>)\n",
      "6.6403489112854\n",
      "0.7713679315751059\n",
      "3 0 tensor(0.3662, grad_fn=<MeanBackward0>)\n",
      "tensor(0.8525) off_val\n",
      "tensor(0.6714) low_val\n",
      "0.8181818 off\n",
      "0.76642334 low\n",
      "3 1 tensor(0.5196, grad_fn=<MeanBackward0>)\n",
      "3 2 tensor(0.3953, grad_fn=<MeanBackward0>)\n",
      "3 3 tensor(0.4364, grad_fn=<MeanBackward0>)\n",
      "3 4 tensor(0.3294, grad_fn=<MeanBackward0>)\n",
      "3 5 tensor(0.7757, grad_fn=<MeanBackward0>)\n",
      "3 6 tensor(0.2112, grad_fn=<MeanBackward0>)\n",
      "3 7 tensor(0.4089, grad_fn=<MeanBackward0>)\n",
      "3 8 tensor(0.5120, grad_fn=<MeanBackward0>)\n",
      "3 9 tensor(0.3619, grad_fn=<MeanBackward0>)\n",
      "3 10 tensor(0.4747, grad_fn=<MeanBackward0>)\n",
      "3 11 tensor(0.1414, grad_fn=<MeanBackward0>)\n",
      "3 12 tensor(0.8152, grad_fn=<MeanBackward0>)\n",
      "3 13 tensor(0.4497, grad_fn=<MeanBackward0>)\n",
      "3 14 tensor(0.4018, grad_fn=<MeanBackward0>)\n",
      "3 15 tensor(0.4163, grad_fn=<MeanBackward0>)\n",
      "3 16 tensor(0.4371, grad_fn=<MeanBackward0>)\n",
      "3 17 tensor(0.5172, grad_fn=<MeanBackward0>)\n",
      "3 18 tensor(0.1211, grad_fn=<MeanBackward0>)\n",
      "6.678213834762573\n",
      "0.6849897982258546\n",
      "4 0 tensor(0.4295, grad_fn=<MeanBackward0>)\n",
      "tensor(0.6230) off_val\n",
      "tensor(0.8571) low_val\n",
      "0.7030303 off\n",
      "0.93430656 low\n",
      "4 1 tensor(0.5909, grad_fn=<MeanBackward0>)\n",
      "4 2 tensor(0.5124, grad_fn=<MeanBackward0>)\n",
      "4 3 tensor(0.5087, grad_fn=<MeanBackward0>)\n",
      "4 4 tensor(0.6550, grad_fn=<MeanBackward0>)\n",
      "4 5 tensor(0.6589, grad_fn=<MeanBackward0>)\n",
      "4 6 tensor(0.4961, grad_fn=<MeanBackward0>)\n",
      "4 7 tensor(0.3341, grad_fn=<MeanBackward0>)\n",
      "4 8 tensor(0.3722, grad_fn=<MeanBackward0>)\n",
      "4 9 tensor(0.2876, grad_fn=<MeanBackward0>)\n",
      "4 10 tensor(0.3380, grad_fn=<MeanBackward0>)\n",
      "4 11 tensor(0.2736, grad_fn=<MeanBackward0>)\n",
      "4 12 tensor(0.3905, grad_fn=<MeanBackward0>)\n",
      "4 13 tensor(0.1489, grad_fn=<MeanBackward0>)\n",
      "4 14 tensor(0.1907, grad_fn=<MeanBackward0>)\n",
      "4 15 tensor(0.5582, grad_fn=<MeanBackward0>)\n",
      "4 16 tensor(0.2335, grad_fn=<MeanBackward0>)\n",
      "4 17 tensor(0.2447, grad_fn=<MeanBackward0>)\n",
      "4 18 tensor(0.2581, grad_fn=<MeanBackward0>)\n",
      "7.259522914886475\n",
      "0.6267466766269584\n",
      "5 0 tensor(0.5120, grad_fn=<MeanBackward0>)\n",
      "tensor(0.7541) off_val\n",
      "tensor(0.7857) low_val\n",
      "0.8060606 off\n",
      "0.8686131 low\n",
      "5 1 tensor(0.1997, grad_fn=<MeanBackward0>)\n",
      "5 2 tensor(0.4343, grad_fn=<MeanBackward0>)\n",
      "5 3 tensor(0.1188, grad_fn=<MeanBackward0>)\n",
      "5 4 tensor(0.1181, grad_fn=<MeanBackward0>)\n",
      "5 5 tensor(0.2831, grad_fn=<MeanBackward0>)\n",
      "5 6 tensor(0.6064, grad_fn=<MeanBackward0>)\n",
      "5 7 tensor(0.1784, grad_fn=<MeanBackward0>)\n",
      "5 8 tensor(0.3675, grad_fn=<MeanBackward0>)\n",
      "5 9 tensor(0.5435, grad_fn=<MeanBackward0>)\n",
      "5 10 tensor(0.3752, grad_fn=<MeanBackward0>)\n",
      "5 11 tensor(0.1138, grad_fn=<MeanBackward0>)\n",
      "5 12 tensor(0.2479, grad_fn=<MeanBackward0>)\n",
      "5 13 tensor(0.1124, grad_fn=<MeanBackward0>)\n",
      "5 14 tensor(0.7378, grad_fn=<MeanBackward0>)\n",
      "5 15 tensor(0.3246, grad_fn=<MeanBackward0>)\n",
      "5 16 tensor(0.3067, grad_fn=<MeanBackward0>)\n",
      "5 17 tensor(0.2352, grad_fn=<MeanBackward0>)\n",
      "5 18 tensor(0.5027, grad_fn=<MeanBackward0>)\n",
      "8.49974274635315\n",
      "0.5777086626672954\n",
      "6 0 tensor(0.2870, grad_fn=<MeanBackward0>)\n",
      "tensor(0.7705) off_val\n",
      "tensor(0.8429) low_val\n",
      "0.8545455 off\n",
      "0.91240877 low\n",
      "6 1 tensor(0.2311, grad_fn=<MeanBackward0>)\n",
      "6 2 tensor(0.3105, grad_fn=<MeanBackward0>)\n",
      "6 3 tensor(0.1447, grad_fn=<MeanBackward0>)\n",
      "6 4 tensor(0.1183, grad_fn=<MeanBackward0>)\n",
      "6 5 tensor(0.2323, grad_fn=<MeanBackward0>)\n",
      "6 6 tensor(0.9885, grad_fn=<MeanBackward0>)\n",
      "6 7 tensor(0.3998, grad_fn=<MeanBackward0>)\n",
      "6 8 tensor(0.3709, grad_fn=<MeanBackward0>)\n",
      "6 9 tensor(0.3828, grad_fn=<MeanBackward0>)\n",
      "6 10 tensor(0.7267, grad_fn=<MeanBackward0>)\n",
      "6 11 tensor(0.3754, grad_fn=<MeanBackward0>)\n",
      "6 12 tensor(0.2684, grad_fn=<MeanBackward0>)\n",
      "6 13 tensor(0.2947, grad_fn=<MeanBackward0>)\n",
      "6 14 tensor(0.5348, grad_fn=<MeanBackward0>)\n",
      "6 15 tensor(0.3701, grad_fn=<MeanBackward0>)\n",
      "6 16 tensor(0.2798, grad_fn=<MeanBackward0>)\n",
      "6 17 tensor(0.6571, grad_fn=<MeanBackward0>)\n",
      "6 18 tensor(0.2895, grad_fn=<MeanBackward0>)\n",
      "8.21356201171875\n",
      "0.5497820139267391\n",
      "7 0 tensor(0.5650, grad_fn=<MeanBackward0>)\n",
      "tensor(0.6393) off_val\n",
      "tensor(0.9286) low_val\n",
      "0.7151515 off\n",
      "0.94160587 low\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/45/wj5m3mk56b38wrkpv32mr0240000gn/T/ipykernel_84542/27498654.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/45/wj5m3mk56b38wrkpv32mr0240000gn/T/ipykernel_84542/3758295448.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/45/wj5m3mk56b38wrkpv32mr0240000gn/T/ipykernel_84542/343772498.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "import time\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    for i, (x, y) in enumerate(dataloader_train):\n",
    "        x = x/255.\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        model.train()\n",
    "        \n",
    "        model.zero_grad()\n",
    "        start = time.time()\n",
    "        \n",
    "        out = model(x.float())\n",
    "        \n",
    "        loss = loss_function(out, y.long())\n",
    "        loss_mean = loss.mean()\n",
    "        loss_mean.backward()\n",
    "        optim.step()\n",
    "\n",
    "        print(epoch, i, loss_mean)\n",
    "        \n",
    "        losses.append(loss.detach().item())\n",
    "        \n",
    "        if epoch % 1 == 0 and epoch != 0 and i == 0:\n",
    "            evaluate(dataloader_val_off, \"off_val\")\n",
    "            evaluate(dataloader_low_val, \"low_val\")\n",
    "            \n",
    "            evaluate2(dataloader_off, \"off\")\n",
    "            evaluate2(dataloader_low, \"low\")\n",
    "            \n",
    "            save_model2(epoch)\n",
    "            #evaluate(dataloader_off, \"off\")\n",
    "            #evaluate(dataloader_low, \"low\")\n",
    "            #evaluate(dataloader_on, \"on\")\n",
    "            #evaluate(dataloader_night, \"night\")\n",
    "            \n",
    "    stop = time.time()\n",
    "    print(\"time: \", stop - start)\n",
    "    print(\"loss: \", np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff0e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
